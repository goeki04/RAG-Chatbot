services:
  nginx:
    build: .
    image: customopenresty
    container_name: nginx_lb
    ports:
      - "8080:80"
    volumes:
      - ./nginx.conf:/usr/local/openresty/nginx/conf/nginx.conf:ro
    environment:
      - AZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}
      - no_proxy=chatbot,rag_chatbot,vdb_1,vdb_2,localhost,127.0.0.1
    depends_on:
      - vdb_1
      - vdb_2
      - chatbot
    networks:
      - qdrant_net

  vdb_1:
    image: qdrant/qdrant:latest
    container_name: vdb_1
    networks:
      - qdrant_net

  vdb_2:
    image: qdrant/qdrant:latest
    container_name: vdb_2
    networks:
      - qdrant_net

  qdrant_loader:
    build:
      context: .
      dockerfile: Dockerfile.loader
    volumes:
      - ./data:/app/data:ro
    environment:
      - no_proxy=vdb_1,vdb_2,localhost,127.0.0.1
    networks:
      - qdrant_net
    restart: on-failure

  ollama:
      image: ollama/ollama:latest
      container_name: ollama
      volumes:
        - ollama_storage:/root/.ollama
        - /etc/ssl/certs/your-proxy-ca.crt:/usr/local/share/ca-certificates/proxy-ca.crt
      networks:
        - qdrant_net
      ports:
        - "11434:11434"
      environment:
        - no_proxy=localhost,127.0.0.1,ollama,chatbot,vdb_1,vdb_2

  chatbot:
    image: chatbot
    build:
      dockerfile: Dockerfile.chatbot
    volumes:
      - ./app.py:/app/app.py
      - fastembed_data:/tmp/fastembed_cache
      - ./data/db1:/app/data/db1
      - ./data/db2:/app/data/db2
    command: streamlit run app.py --server.address=0.0.0.0 --server.baseUrlPath=/chat/ --server.fileWatcherType poll
    environment:
      - QDRANT_URL=http://vdb_1:6333
      - COLLECTION_NAME=datenbank_eins
      - OLLAMA_URL=http://ollama:11434
      - STREAMLIT_SERVER_BASE_PATH=/chat
      - no_proxy=vdb_1,vdb_2,ollama,chatbot,localhost,127.0.0.1
    depends_on:
      - vdb_1
      - vdb_2
      - ollama
    networks:
      - qdrant_net
    restart: unless-stopped
volumes:
  fastembed_data:
  ollama_storage:
networks:
  qdrant_net:
    driver: bridge

